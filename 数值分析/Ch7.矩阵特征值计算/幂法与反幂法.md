# 幂法

幂法其实我感觉就是一个点：
$$\lim_{k\to\infty}A^k=\lambda_\max A^{k-1}$$
实际上的推导也是这个意思：
$$\begin{cases}
v_1=Av_0\\
v_2=Av_1=A^2v_0\\
\cdots\\
v_n=A^{n}v_0
\end{cases}$$
如果这个矩阵是完备的，那么其对应的线性变换也不会改变其维度大小，那么从基变化的角度来看的话，这个其实可以对向量 $v_0$ 做特征向量上的坐标变换：
$$v_0=\alpha_1x_1+\alpha_2x_2+\cdots+\alpha_nx_n$$
那么就又会线性变换的结果：
$$v_k=Av_{k-1}=A^kv=\alpha_{1}\lambda_1^{k}x_1+\alpha_{2}\lambda_2^{k}x_2+\cdots+\alpha_{n}\lambda_n^{k}x_n$$
如果 $\lambda_1$ 是最大的话，那么就有：
$$v_k=\lambda_1^k(\alpha_1x_1+\sum_{i=2}^{n}(\frac{\lambda_{i}}{\lambda_1})^k\alpha_ix_i)$$
如果做以下操作：
$$\frac{v_k}{v_{k-1}}=\lambda_1\frac{(\alpha_1x_1+\sum_{i=2}^{n}(\frac{\lambda_{i}}{\lambda_1})^k\alpha_ix_i)}{(\alpha_1x_1+\sum_{i=2}^{n}(\frac{\lambda_{i}}{\lambda_1})^{k-1}\alpha_ix_i)}$$
因为当 $k\to\infty$ 时，$(\frac{\lambda_{i}}{\lambda_1})^k\to0$ ，那么后者将趋近于 $\lambda_1$ ，而得到了 $\lambda_1$，则也可以得到：
$$\lim_{k\to\infty}\frac{v_k}{\lambda_1^k}=\alpha_1x_1$$
后者就是特征向量

不过这个方法也有一些局限性，具体来时就是，当第二大特征值和第一大特征值很近的话，迭代次数会非常大，那么也就会有相应的解决措施

# 反幂法

反幂法其实和幂法是一样的，因为他就是求解 $A^{-1}$ 的最大特征值然后再去取倒数
[[矩阵特征值]]