如果说函数空间是线性空间拓展至函数的一种衡量误差大小的一种工具。**向量和矩阵的范数** 是线性空间中用来度量大小、误差或收敛性的工具。在数值分析中，尤其是迭代算法中，范数用于控制误差的传播与估计收敛速度。
# 向量范数

这个实际上是高等代数的内容，向量的范数本质上就是一个向量对另外一个向量做坐标相乘，这个部分其实可以和线性变换相关，这里不做过多赘述，感兴趣可以去看
https://www.bilibili.com/video/BV1ys411472E/?p=10&share_source=copy_web&vd_source=9cc7c439d7b473f4ca738607e705e34b

因而可以确定两向量的内积：
$$(x,y)=y^Tx=\sum x_iy_i$$
而其欧式距离为该向量自身点积结果的开方项：
$$||x||=(x,x)^{\frac12}=\sqrt{\sum x_ix_i}$$
其满足线性性质，其中包括：

| $\|u\|\geqslant0$                     |
| ------------------------------------- |
| $\|u\|=0\Leftrightarrow u={0}$        |
| $\|\lambda u\|=\|\lambda\|\cdot\|u\|$ |
| $\|u+v\|\leqslant\|u\|+\|v\|$         |
当然，也有与数学分析中交叉的内容，即 Cauchy–Schwarz 不等式，是**内积空间**中度量“夹角”与“正交性”的基础
$$|(x,y)|\leqslant||x||·||y||$$
当然除去内积空间，其仍然存在所谓的赋范空间中范数的定义，即：
$$\begin{align}
||x||_\infty&=\max_{1\leqslant i\leqslant n}|x_i|\\
||x||_2&=\sqrt{\sum_{i=1}^nx_i^2}\\
||x||_1&=\sum_{i=1}^n|x_i|
\end{align}$$
值得注意的是，范数的定义应该是等价的，也就是说，不会因为`x-`范数的选取，导致两个在前一个范数空间内更大的向量，在新的范数定义下比另一个小：
$\exists~c_1,c_2>0$，对于 $\forall~x\in\mathbb R^n$，都会有：
$$
c_1||x||_s\leqslant||x||_t\leqslant c_2||x||_s
$$
**在有限维空间中，所有范数彼此等价**，也就是说，用不同范数度量向量，其大小的差异只是常数倍
注意，这里很重要一点是有限维，在无限维，以 Hilbert 空间为例，其是失效的

# 向量收敛

分量按范数逐渐收敛于目标元素
$$x^{(k)}=[x^{(k)}_1,x^{(k)}_2,\cdots,x^{(k)}_n]$$
$$x^{*}=[x^{*}_1,x^{*}_2,\cdots,x^{*}_n]$$
从中各分量有：
$$\lim_{k\to\infty}x^{(k)}=x^*\Leftrightarrow \lim_{k\to\infty}||x^{(k)}-x^*||=0$$
由于范数的等价性质，这里范数可以任取，证明过程与数学分析中的极限证明类似，这里只证明右至左：
$$\begin{align}
&\lim_{k\to\infty}||x^{(k)}-x^*||=0 \\
&\Rightarrow \forall~\epsilon >0,\exists~N,s.t.||x^{(k)}-x^*||<\epsilon\\
&\Rightarrow \sqrt{\sum_{i=1}^n|x_i^{(k)}-x_i^*|^2}=0\\
&\Rightarrow \forall~i,|x_i^{(k)}-x_i^*|<\epsilon\\
&\Rightarrow k\to\infty,x_{i}^{(k)}\to x_{i}^*
\end{align}$$
# 矩阵范数

如果将向量范数的概念推广到矩阵上，也就是视 $\mathbb R^{n\times n}$ 中的矩阵为 $\mathbb R^{n}$ 中的向量，因此会有：
$$||A||_F=\sqrt{\sum_{i,j=1}^{n}a_{i,j}^2}$$
这个被称作 A 的 Frobenius 范数，可视为将矩阵看作一个向量后的 2-范数

而更多用的较为宽泛的矩阵范数为：

**算子范数**
$$||A||_v=\max_{x\neq 0}\frac{||Ax||_v}{||x||_v}$$
也就是最大的矩阵奇异值

$$\begin{align}
||A||_{\infty}&=\max_{1\leqslant i\leqslant n}\sum_{j=1}^{n}|a_ij|\to行范数\\
||A||_{1}&=\max_{1\leqslant j\leqslant n}\sum_{i=1}^{n}|a_ij|\to列范数\\
||A||_{2}&=\sqrt{\lambda_{\max}(A^T·A)}
\end{align}$$
关于无穷和1-范数的记忆，其实可以这么理解，向量习惯主要是列向量为主，因此靠前顺位的应该是列向量绝对值求和的最大值，而行向量则为其反向，因此会有无穷级
$\lambda_{\max}(A^T·A)$ 表示 $A^T·A$ 的最大特征值，和向量的2-范数一致，其本质也是 $X^T·X$ 开根

矩阵的谱半径 Spectral radius of matrix
谱 $\sigma$ 为所有特征值的集合
而谱半径 $\rho$ 就是其中的最大值，也就是最大特征值
$$\rho(A)=\max_{1\le i\le n}|\lambda_i|$$
其是衡量线性变换发散速度的关键指标

谱半径和矩阵范数是两个东西，其并不相通，其显著区别在于其表现了矩阵的不同性质

一般性关系：**谱半径始终小于等于矩阵范数**

而对称矩阵 $A$ ，会有：
$$\rho(A)=||A||_2$$
[[矩阵 Intro]]
[[函数空间]]


